{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Rm3eAtHw9BVL",
        "outputId": "52f8bf95-6ba8-432f-b443-941008b3f5e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.7.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"
          ]
        }
      ],
      "source": [
        "pip install requests beautifulsoup4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_NKceG4uFDtf"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import urllib3\n",
        "\n",
        "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
        "\n",
        "def get_project_details(detail_url):\n",
        "    detail_page = requests.get(detail_url, verify=False)\n",
        "    detail_soup = BeautifulSoup(detail_page.content, 'html.parser')\n",
        "\n",
        "    project_details = {}\n",
        "    project_details['GSTIN No'] = detail_soup.find('span', id='ContentPlaceHolder1_lblGSTIN').text.strip()\n",
        "    project_details['PAN No'] = detail_soup.find('span', id='ContentPlaceHolder1_lblPAN').text.strip()\n",
        "    project_details['Name'] = detail_soup.find('span', id='ContentPlaceHolder1_lblPromoterName').text.strip()\n",
        "    project_details['Permanent Address'] = detail_soup.find('span', id='ContentPlaceHolder1_lblPermAddress').text.strip()\n",
        "\n",
        "    return project_details\n",
        "\n",
        "def scrape_projects():\n",
        "    url = 'https://hprera.nic.in/PublicDashboard'\n",
        "    page = requests.get(url, verify=False) \n",
        "    soup = BeautifulSoup(page.content, 'html.parser')\n",
        "\n",
        "    project_links = []\n",
        "    for link in soup.find_all('a', href=True, string='RERA No', limit=6):\n",
        "        project_links.append(link['href'])\n",
        "\n",
        "    project_details_list = []\n",
        "    for link in project_links:\n",
        "        detail_url = 'https://hprera.nic.in/' + link\n",
        "        project_details = get_project_details(detail_url)\n",
        "        project_details_list.append(project_details)\n",
        "\n",
        "    return project_details_list\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    projects = scrape_projects()\n",
        "    for i, project in enumerate(projects, 1):\n",
        "        print(f\"Project {i}:\")\n",
        "        for key, value in project.items():\n",
        "            print(f\"{key}: {value}\")\n",
        "        print(\"\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
